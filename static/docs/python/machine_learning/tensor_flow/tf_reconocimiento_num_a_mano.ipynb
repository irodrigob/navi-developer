{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1596051010845",
   "display_name": "Python 3.7.7 64-bit ('test': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Using TensorFlow backend.\n"
    }
   ],
   "source": [
    "# Libreria tensor flowe\n",
    "import tensorflow as tf\n",
    "# Daset de datos con números escritos a manorom \n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow_datasets as tfds\n",
    "# API de tensorfow para construir y entrenar modelos de alto nivel\n",
    "from tensorflow import keras\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Se recuperan los datos y el metadata de los datos\n",
    "#mnist_train2, mnist_info = tfds.load('mnist', split='train', as_supervised=True,  shuffle_files=True,with_info=True)\n",
    "# De los datos, se extrae los datos de entrenamiento y test\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.reshape(x_train.shape[0], 784)\n",
    "x_test = x_test.reshape(x_test.shape[0], 784)\n",
    "\n",
    "# El formato original esta en \"unit8\" con lo que se tiene que convertir en un float32 para que funcione\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "#Se divide la imagen en 255 porque es la escala de grises y es mejor operar con 0 y 1. Más sencillo\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "# Paso necesario porque devuelve los registros y toda la información en la misma columna. Pero \n",
    "# se necesita esos valores esten en un matriz de 10 columnas para calcular bien los acietros.\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)\n",
    "\n",
    "mnist_train = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# El ejemplo de .map es lo mismo que se ha hecho arriba cuando se han leido todos los datos\n",
    "# El TFDS devuelve las imagenes en uint8, mientras que la red espera float 32\n",
    "# hay que normalizar para que funcione\n",
    "#def normalize_img(image, label):\n",
    "#  \"\"\"Normalizes images: `uint8` -> `float32`.\"\"\"\n",
    "#  return tf.cast(image, tf.float32) / 255, label\n",
    "#\n",
    "# map permite realizar una transformación a cada uno de los datos del conjuntos de datos\n",
    "# que lo compone. En este caso lo que se se hace convertir las imagenes del TDFS al formato\n",
    "# compatible de la red.\n",
    "#mnist_train = mnist_train.map(\n",
    "#    normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "# Activa que los datos se guarden cache para mejorar el rendimiento\n",
    "mnist_train = mnist_train.cache()\n",
    "# Activa que los datos se recuperan de manera aleatoria\n",
    "#mnist_train = mnist_train.shuffle(mnist_info.splits['train'].num_examples)\n",
    "# Número de lotes que hará en cada iteracción\n",
    "mnist_train = mnist_train.batch(100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Imagen del número descompuesto a un vector. Se informa que el vector es de 784 porque es\n",
    "# multiplicación de 28 x 28 que son los pixeles de la imagen\n",
    "# El valor None es para indicar que el valor, o filas, son variables según el número de imagenes\n",
    "x = tf.keras.Input(name='x', shape=(None,784), dtype=tf.float32) \n",
    "# Matriz de pesos, que se inicializa a 0, con el tf.zeros. \n",
    "# La matriz tiene 784 filas, que son el número de pixeles. Y 10 columnas porque\n",
    "# la siguienet capa tiene 10 elementos. La siguiente capa es la de salida\n",
    "P = tf.Variable(tf.zeros([784,10]))\n",
    "# Son los bias, o sesgo. Será un vector de 10 elementos inicializado a 0.\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "# Es la operación de salida. Aquí es la formula que se explica en la parte de redes\n",
    "# neuronales, como funciona. \n",
    "# la formula es la multiplicación de la matriz de entrada * peso, más el sesgo.\n",
    "y = tf.matmul(x,P)+b\n",
    "# Matriz con las etiquetas reales del set de datos. Que tendrá un nuevo variable de filas\n",
    "# y 10 columnas.\n",
    "yR = tf.keras.Input(name='yR', shape=(None,10), dtype=tf.float32) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "x  (None, None, 784)\nP  (784, 10)\nb  (10,)\nyr  (None, None, 10)\ny  (None, None, 10)\n"
    }
   ],
   "source": [
    "print(\"x \", x.shape)\n",
    "print(\"P \", P.shape)\n",
    "print(\"b \", b.shape)\n",
    "\n",
    "print(\"yr \", yR.shape)\n",
    "print(\"y \", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss():\n",
    "    log_x = tf.math.log(x)\n",
    "    m = tf.math.square(log_x)\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcion para ver como esta aprendiendo el algoritmo\n",
    "def avance(epoca_i,X_data, Y_data):\n",
    "    x = X_data\n",
    "    yR = Y_data\n",
    "          \n",
    "    y = tf.matmul(X_data,P)+b    \n",
    "\n",
    "    #print(\"x \", x.shape)\n",
    "    #print(\"P \", P.shape)\n",
    "    #print(\"b \", b.shape)\n",
    "\n",
    "    #print(\"yr \", Y_data.shape)\n",
    "    #print(\"y \", y.shape)\n",
    "\n",
    "    # Se definen los algoritmos de como va aprender la red en cada una de las iteracciones\n",
    "    # Softmax nos va dar un vector de 10 elementos para generar la predicción pero en probabilidades\n",
    "    # Esto nos va a dar un vector donde en cada posición nos va dar el % de probalidades que sea.\n",
    "    # Es la libreria es la que nos va dar el error de nuestra predicciones. Se le pasará las \n",
    "    # etiquetas de los números y la salida de la red para que nos diga que tan bien o tan mal esta\n",
    "    # el algoritmo para repartir los pesos.\n",
    "    softmax = tf.nn.softmax_cross_entropy_with_logits(labels=Y_data,logits=y)\n",
    "    \n",
    "    # Es la función del coste de la predicción este número tiene que tender a 0.\n",
    "    costo = tf.reduce_mean(softmax)\n",
    "    \n",
    "    # Código original tf.train.GradientDescentOptimizer(0.5).minimize(costo)\n",
    "    # Es el que permitir ajustar los pesos de nuestra matriz. Es decir, si el costo = 2, ajustará los pesos\n",
    "    # para que ese costo tienda a 0.\n",
    "    optimizador = tf.keras.optimizers.SGD(learning_rate=0.5,momentum=0.0, nesterov=False) #.minimize(compute_loss,costo)\n",
    "    print(optimizador)\n",
    "\n",
    "    # Da el arreglo de booleanos para decirnos cuales estan bien y cuales están mal\n",
    "    # argmax nos va dar el valor más alto de todas las predicciones que hizo softmax. \n",
    "    # Y se va comparar con los datos reales para decirnos si se hizo bien o mal la\n",
    "    # predicción\n",
    "    #prediccion = tf.equal(tf.argmax(y,1),tf.argmax(yR,1))\n",
    "\n",
    "    # Da el % sobre el arreglo  de predicción. Si en la variable de predicción hay\n",
    "    # 10 elementos, 5 están bien y 5 están mal. El % será edl 50%\n",
    "    #accuracy = tf.reduce_mean(tf.cast(prediccion, tf.float32))\n",
    "\n",
    "    #if(epoca_i%50==0):\n",
    "    #    print('Epoca: {:<4} - Costo: {:<8.3} Certeza: {:<5.3}'.format(epoca_i,costo,accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x000001AE9AC50BC8>\n<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x000001AE9ABF2A08>\n<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x000001AE9AC4C7C8>\n<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x000001AE9ABF2A08>\n<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x000001AE9AC4C7C8>\n<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x000001AE9ABF2A08>\n<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x000001AE9B3856C8>\n<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x000001AE9ABB29C8>\n<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x000001AE9B3856C8>\n<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x000001AE9A979E08>\n"
    }
   ],
   "source": [
    "for epoch in range(1):       \n",
    "    ds = mnist_train.take(10)\n",
    "    for image, label in tfds.as_numpy(ds):        \n",
    "        avance(epoch, image, label)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                                        "
   ]
  }
 ]
}